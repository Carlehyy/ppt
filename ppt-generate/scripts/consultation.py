#!/usr/bin/env python3
"""
Consultation Module â€” æ™ºèƒ½å’¨è¯¢ä¸ä¿¡æ¯å®Œå¤‡æ€§æ£€æŸ¥
PRDè¦æ±‚ï¼š
1. å…³é”®ä¿¡æ¯æ± ç¡®è®¤ â€” å‘ç”¨æˆ·å±•ç¤ºæå–çš„å…³é”®ä¿¡æ¯ï¼Œè¯·æ±‚ç¡®è®¤
2. ä¿¡æ¯å®Œå¤‡æ€§æ£€æŸ¥ â€” è¯†åˆ«ç¼ºå¤±ä¿¡æ¯ï¼Œå¼•å¯¼ç”¨æˆ·è¡¥å……
3. æ±‡æŠ¥æ„å›¾æ¾„æ¸… â€” æ˜ç¡®åœºæ™¯ã€å—ä¼—ã€æ ¸å¿ƒè¯‰æ±‚
"""

import json
from typing import Dict, Any, List, Optional


class ConsultationManager:
    """æ™ºèƒ½å’¨è¯¢ç®¡ç†å™¨"""

    def __init__(self, llm_client=None):
        self.llm_client = llm_client

    def run_consultation(self, parsed_content: Dict, template_analysis: Dict,
                         user_config: Optional[Dict] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„å’¨è¯¢æµç¨‹

        Args:
            parsed_content: å†…å®¹è§£æç»“æœï¼ˆå››å±‚é€’è¿›ï¼‰
            template_analysis: æ¨¡æ¿åˆ†æç»“æœ
            user_config: ç”¨æˆ·å·²æä¾›çš„é…ç½®

        Returns:
            {
                "key_info_pool": {},        # å…³é”®ä¿¡æ¯æ± 
                "information_gaps": [],     # ä¿¡æ¯ç¼ºå£
                "consultation_questions": [],# éœ€è¦å‘ç”¨æˆ·ç¡®è®¤çš„é—®é¢˜
                "auto_decisions": [],       # Agentè‡ªåŠ¨åšå‡ºçš„å†³ç­–
                "final_config": {},         # æœ€ç»ˆé…ç½®ï¼ˆåˆå¹¶ç”¨æˆ·è¾“å…¥å’Œæ¨æ–­ï¼‰
            }
        """
        user_config = user_config or {}

        # Step 1: æ„å»ºå…³é”®ä¿¡æ¯æ± 
        print("  [å’¨è¯¢] æ„å»ºå…³é”®ä¿¡æ¯æ± ...")
        key_info_pool = self._build_key_info_pool(parsed_content)

        # Step 2: ä¿¡æ¯å®Œå¤‡æ€§æ£€æŸ¥
        print("  [å’¨è¯¢] ä¿¡æ¯å®Œå¤‡æ€§æ£€æŸ¥...")
        completeness = self._check_information_completeness(
            parsed_content, template_analysis, user_config
        )

        # Step 3: ç”Ÿæˆå’¨è¯¢é—®é¢˜
        print("  [å’¨è¯¢] ç”Ÿæˆå’¨è¯¢é—®é¢˜...")
        questions = self._generate_consultation_questions(
            key_info_pool, completeness, user_config, template_analysis
        )

        # Step 4: è‡ªåŠ¨å†³ç­–ï¼ˆå¯¹äºå¯æ¨æ–­çš„ä¿¡æ¯ï¼‰
        print("  [å’¨è¯¢] è‡ªåŠ¨å†³ç­–...")
        auto_decisions = self._make_auto_decisions(
            parsed_content, template_analysis, user_config
        )

        # Step 5: åˆå¹¶æœ€ç»ˆé…ç½®
        final_config = self._merge_final_config(user_config, auto_decisions)

        return {
            "key_info_pool": key_info_pool,
            "information_gaps": completeness.get("gaps", []),
            "consultation_questions": questions,
            "auto_decisions": auto_decisions,
            "final_config": final_config,
        }

    def _build_key_info_pool(self, parsed_content: Dict) -> Dict[str, Any]:
        """
        æ„å»ºå…³é”®ä¿¡æ¯æ±  â€” å°†è¯­ä¹‰å•å…ƒæŒ‰ç±»å‹åˆ†ç»„ï¼Œæ ‡æ³¨æ¥æºå’Œç½®ä¿¡åº¦
        """
        semantic_units = parsed_content.get("semantic_units", [])
        framework = parsed_content.get("framework_mapping", {})

        # æŒ‰ç±»å‹åˆ†ç»„
        pool = {
            "background": [],      # èƒŒæ™¯/ç›®æ ‡
            "achievement": [],     # å…³é”®æˆæœ
            "data": [],            # å…³é”®æ•°æ®
            "problem": [],         # é—®é¢˜/é£é™©
            "plan": [],            # ä¸‹ä¸€æ­¥è®¡åˆ’
            "method": [],          # æ–¹æ³•/è¿‡ç¨‹
            "conclusion": [],      # å…³é”®ç»“è®º
        }

        for unit in semantic_units:
            unit_type = unit.get("type", "other")
            if unit_type in pool:
                pool[unit_type].append({
                    "content": unit.get("content", ""),
                    "source": unit.get("source", ""),
                    "confidence": unit.get("confidence", "medium"),
                    "granularity": unit.get("granularity", "should_show"),
                    "key_data": unit.get("key_data", []),
                    "data_validation": unit.get("data_validation", []),
                })

        # ç»Ÿè®¡ä¿¡æ¯
        pool["_stats"] = {
            "total_units": len(semantic_units),
            "must_show": sum(1 for u in semantic_units if u.get("granularity") == "must_show"),
            "has_data_warnings": any(
                u.get("validation_warning") for u in semantic_units
            ),
            "estimated_pages": framework.get("estimated_total_pages", 0),
        }

        return pool

    def _check_information_completeness(self, parsed_content: Dict,
                                         template_analysis: Dict,
                                         user_config: Dict) -> Dict:
        """ä¿¡æ¯å®Œå¤‡æ€§æ£€æŸ¥"""
        gaps = []

        # æ£€æŸ¥PRDè¦æ±‚çš„å¿…è¦ä¿¡æ¯
        # 1. æ±‡æŠ¥åœºæ™¯
        if not user_config.get("scenario"):
            gaps.append({
                "field": "scenario",
                "description": "æ±‡æŠ¥åœºæ™¯æœªæŒ‡å®š",
                "importance": "high",
                "can_infer": True,
            })

        # 2. æ ¸å¿ƒæ„å›¾
        if not user_config.get("core_intent"):
            gaps.append({
                "field": "core_intent",
                "description": "æ ¸å¿ƒæ±‡æŠ¥æ„å›¾æœªæ˜ç¡®",
                "importance": "high",
                "can_infer": True,
            })

        # 3. ç›®æ ‡å—ä¼—
        if not user_config.get("audience"):
            gaps.append({
                "field": "audience",
                "description": "ç›®æ ‡å—ä¼—æœªæŒ‡å®š",
                "importance": "medium",
                "can_infer": True,
            })

        # 4. é¡µæ•°é™åˆ¶
        if not user_config.get("page_limit"):
            gaps.append({
                "field": "page_limit",
                "description": "é¡µæ•°é™åˆ¶æœªæŒ‡å®š",
                "importance": "medium",
                "can_infer": True,
            })

        # 5. æ£€æŸ¥å†…å®¹å±‚é¢çš„ç¼ºå£
        content_gaps = parsed_content.get("information_gaps", [])
        for gap in content_gaps:
            gaps.append({
                "field": "content",
                "description": gap.get("expected_info", ""),
                "importance": "medium",
                "status": gap.get("status", "missing"),
                "suggestion": gap.get("suggestion", ""),
            })

        # 6. æ£€æŸ¥PPTæ ‡é¢˜
        if not user_config.get("presentation_title"):
            gaps.append({
                "field": "presentation_title",
                "description": "PPTæ ‡é¢˜æœªæŒ‡å®š",
                "importance": "high",
                "can_infer": False,
            })

        return {
            "gaps": gaps,
            "completeness_score": max(0, 100 - len(gaps) * 10),
            "critical_gaps": [g for g in gaps if g["importance"] == "high" and not g.get("can_infer")],
        }

    def _generate_consultation_questions(self, key_info_pool: Dict,
                                          completeness: Dict,
                                          user_config: Dict,
                                          template_analysis: Dict) -> List[Dict]:
        """ç”Ÿæˆéœ€è¦å‘ç”¨æˆ·ç¡®è®¤çš„å’¨è¯¢é—®é¢˜"""
        questions = []

        if not self.llm_client:
            return self._generate_fallback_questions(completeness, user_config)

        # å‡†å¤‡ä¸Šä¸‹æ–‡
        stats = key_info_pool.get("_stats", {})
        gaps = completeness.get("gaps", [])
        design_lang = template_analysis.get("design_language", {})

        prompt = (
            "ä½ æ˜¯ä¸“ä¸šçš„PPTå’¨è¯¢é¡¾é—®ã€‚æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œç”Ÿæˆéœ€è¦å‘ç”¨æˆ·ç¡®è®¤çš„é—®é¢˜ã€‚\n\n"
            "## åŸåˆ™\n"
            "1. åªé—®å¿…è¦çš„é—®é¢˜ï¼Œèƒ½æ¨æ–­çš„å°±ä¸é—®\n"
            "2. æä¾›é€‰é¡¹è®©ç”¨æˆ·å¿«é€Ÿé€‰æ‹©\n"
            "3. é—®é¢˜æ•°é‡æ§åˆ¶åœ¨3-5ä¸ª\n"
            "4. æ¯ä¸ªé—®é¢˜éƒ½è¦æœ‰åˆç†çš„é»˜è®¤æ¨è\n\n"
            f"## å·²çŸ¥ä¿¡æ¯\n"
            f"- ç”¨æˆ·å·²æä¾›é…ç½®: {json.dumps(user_config, ensure_ascii=False)}\n"
            f"- ç´ æç»Ÿè®¡: {json.dumps(stats, ensure_ascii=False)}\n"
            f"- ä¿¡æ¯ç¼ºå£: {json.dumps(gaps, ensure_ascii=False)}\n"
            f"- æ¨¡æ¿æ°”è´¨: {design_lang.get('design_temperament', 'æœªçŸ¥')}\n\n"
            "## å¿…é¡»ç¡®è®¤çš„ä¿¡æ¯ï¼ˆå¦‚æœç”¨æˆ·æœªæä¾›ï¼‰\n"
            "1. æ±‡æŠ¥åœºæ™¯ï¼ˆå·¥ä½œæ€»ç»“/é¡¹ç›®è¿›å±•/æ•°æ®åˆ†æ/æ–¹æ¡ˆææ¡ˆï¼‰\n"
            "2. æ ¸å¿ƒæ„å›¾ï¼ˆå±•ç¤ºæˆæœ/åˆ†æé—®é¢˜/æå‡ºæ–¹æ¡ˆ/äº‰å–èµ„æºï¼‰\n"
            "3. ç›®æ ‡å—ä¼—ï¼ˆç›´å±é¢†å¯¼/é«˜å±‚ç®¡ç†/å®¢æˆ·/å›¢é˜Ÿæˆå‘˜ï¼‰\n"
            "4. é¡µæ•°åå¥½\n"
            "5. å…³é”®ä¿¡æ¯ç¡®è®¤ï¼ˆæ˜¯å¦æœ‰éœ€è¦ç‰¹åˆ«å¼ºè°ƒæˆ–åˆ é™¤çš„å†…å®¹ï¼‰\n\n"
            "è¯·è¾“å‡ºJSON:\n"
            '{"questions": [\n'
            '  {"id": 1, "field": "scenario", "question": "é—®é¢˜æ–‡æœ¬", '
            '"type": "single_choice", "options": ["é€‰é¡¹1", "é€‰é¡¹2"], '
            '"default": "æ¨èé€‰é¡¹", "reason": "æ¨èç†ç”±", '
            '"skip_if": "å¦‚æœç”¨æˆ·å·²æä¾›åˆ™è·³è¿‡"}\n'
            "]}"
        )
        try:
            result = self.llm_client.call_llm(prompt, response_json=True)
            questions = result.get("questions", [])
        except Exception as e:
            print(f"    âš  ç”Ÿæˆå’¨è¯¢é—®é¢˜å¤±è´¥: {e}")
            questions = self._generate_fallback_questions(completeness, user_config)

        # è¿‡æ»¤æ‰ç”¨æˆ·å·²ç»æä¾›çš„ä¿¡æ¯
        filtered = []
        for q in questions:
            field = q.get("field", "")
            if field and user_config.get(field):
                continue
            filtered.append(q)

        return filtered

    def _generate_fallback_questions(self, completeness: Dict, user_config: Dict) -> List[Dict]:
        """å½“LLMä¸å¯ç”¨æ—¶çš„å…œåº•é—®é¢˜"""
        questions = []
        if not user_config.get("scenario"):
            questions.append({
                "id": 1, "field": "scenario",
                "question": "è¿™æ˜¯ä»€ä¹ˆç±»å‹çš„æ±‡æŠ¥ï¼Ÿ",
                "type": "single_choice",
                "options": ["å·¥ä½œæ€»ç»“æ±‡æŠ¥", "é¡¹ç›®è¿›å±•æ±‡æŠ¥", "æ•°æ®åˆ†ææ±‡æŠ¥", "æ–¹æ¡ˆææ¡ˆæ±‡æŠ¥"],
                "default": "å·¥ä½œæ€»ç»“æ±‡æŠ¥",
            })
        if not user_config.get("core_intent"):
            questions.append({
                "id": 2, "field": "core_intent",
                "question": "æ‚¨å¸Œæœ›é€šè¿‡è¿™ä»½PPTä¼ è¾¾ä»€ä¹ˆæ ¸å¿ƒä¿¡æ¯ï¼Ÿ",
                "type": "single_choice",
                "options": ["å±•ç¤ºæˆæœ", "åˆ†æé—®é¢˜", "æå‡ºæ–¹æ¡ˆ", "äº‰å–èµ„æº"],
                "default": "å±•ç¤ºæˆæœ",
            })
        if not user_config.get("page_limit"):
            questions.append({
                "id": 3, "field": "page_limit",
                "question": "æœŸæœ›çš„é¡µæ•°èŒƒå›´ï¼Ÿ",
                "type": "single_choice",
                "options": ["10-15é¡µ", "15-20é¡µ", "20-30é¡µ"],
                "default": "15-20é¡µ",
            })
        return questions

    def _make_auto_decisions(self, parsed_content: Dict, template_analysis: Dict,
                              user_config: Dict) -> List[Dict]:
        """å¯¹äºå¯æ¨æ–­çš„ä¿¡æ¯ï¼Œè‡ªåŠ¨åšå‡ºå†³ç­–"""
        decisions = []
        profiles = parsed_content.get("document_profiles", [])
        framework = parsed_content.get("framework_mapping", {})
        design_lang = template_analysis.get("design_language", {})

        # 1. æ¨æ–­æ±‡æŠ¥åœºæ™¯
        if not user_config.get("scenario") and profiles:
            doc_types = [p.get("doc_type", "") for p in profiles]
            if any("æ€»ç»“" in t for t in doc_types):
                inferred = "å·¥ä½œæ€»ç»“æ±‡æŠ¥"
            elif any("é¡¹ç›®" in t for t in doc_types):
                inferred = "é¡¹ç›®è¿›å±•æ±‡æŠ¥"
            elif any("æ•°æ®" in t or "åˆ†æ" in t for t in doc_types):
                inferred = "æ•°æ®åˆ†ææ±‡æŠ¥"
            else:
                inferred = "å·¥ä½œæ€»ç»“æ±‡æŠ¥"
            decisions.append({
                "field": "scenario",
                "value": inferred,
                "confidence": "medium",
                "reason": f"æ ¹æ®æ–‡æ¡£ç±»å‹({', '.join(doc_types)})æ¨æ–­",
            })

        # 2. æ¨æ–­æ ¸å¿ƒæ„å›¾
        if not user_config.get("core_intent") and profiles:
            natures = [p.get("info_nature", "") for p in profiles]
            if any("æˆæœ" in n for n in natures):
                inferred = "å±•ç¤ºæˆæœ"
            elif any("é—®é¢˜" in n for n in natures):
                inferred = "åˆ†æé—®é¢˜"
            elif any("è§„åˆ’" in n or "å»ºè®®" in n for n in natures):
                inferred = "æå‡ºæ–¹æ¡ˆ"
            else:
                inferred = "å±•ç¤ºæˆæœ"
            decisions.append({
                "field": "core_intent",
                "value": inferred,
                "confidence": "medium",
                "reason": f"æ ¹æ®å†…å®¹æ€§è´¨({', '.join(natures)})æ¨æ–­",
            })

        # 3. æ¨æ–­é¡µæ•°
        if not user_config.get("page_limit"):
            estimated = framework.get("estimated_total_pages", 15)
            decisions.append({
                "field": "page_limit",
                "value": max(10, min(30, estimated)),
                "confidence": "medium",
                "reason": f"æ ¹æ®å†…å®¹é‡ä¼°ç®—çº¦{estimated}é¡µ",
            })

        # 4. æ¨æ–­è¯­è¨€é£æ ¼
        if not user_config.get("language_style"):
            temperament = design_lang.get("design_temperament", "")
            formality = design_lang.get("formality_level", "æ­£å¼")
            if "æ´»æ³¼" in temperament or "åˆ›æ„" in temperament:
                style = "ç”ŸåŠ¨æ´»æ³¼"
            elif "å­¦æœ¯" in temperament or "ä¸¥è°¨" in temperament:
                style = "ä¸“ä¸šä¸¥è°¨"
            else:
                style = "ç®€æ´æ˜å¿«"
            decisions.append({
                "field": "language_style",
                "value": style,
                "confidence": "medium",
                "reason": f"æ ¹æ®æ¨¡æ¿æ°”è´¨({temperament})å’Œæ­£å¼ç¨‹åº¦({formality})æ¨æ–­",
            })

        return decisions

    def _merge_final_config(self, user_config: Dict, auto_decisions: List[Dict]) -> Dict:
        """åˆå¹¶ç”¨æˆ·é…ç½®å’Œè‡ªåŠ¨å†³ç­–ï¼Œç”¨æˆ·é…ç½®ä¼˜å…ˆ"""
        final = dict(user_config)
        for decision in auto_decisions:
            field = decision["field"]
            if field not in final or not final[field]:
                final[field] = decision["value"]
                final[f"_{field}_source"] = "auto_inferred"
                final[f"_{field}_confidence"] = decision["confidence"]
        return final

    def format_consultation_output(self, consultation_result: Dict) -> str:
        """
        å°†å’¨è¯¢ç»“æœæ ¼å¼åŒ–ä¸ºäººç±»å¯è¯»çš„æ–‡æœ¬
        ä¾›AIåŠ©æ‰‹å±•ç¤ºç»™ç”¨æˆ·
        """
        lines = []
        pool = consultation_result.get("key_info_pool", {})
        stats = pool.get("_stats", {})
        questions = consultation_result.get("consultation_questions", [])
        auto_decisions = consultation_result.get("auto_decisions", [])
        gaps = consultation_result.get("information_gaps", [])

        # 1. ç´ æåˆ†ææ‘˜è¦
        lines.append("## ğŸ“‹ ç´ æåˆ†ææ‘˜è¦\n")
        lines.append(f"- å…±æå– **{stats.get('total_units', 0)}** ä¸ªä¿¡æ¯è¦ç´ ")
        lines.append(f"- å…¶ä¸­ **{stats.get('must_show', 0)}** ä¸ªä¸ºå¿…é¡»å‘ˆç°çš„æ ¸å¿ƒä¿¡æ¯")
        if stats.get("has_data_warnings"):
            lines.append("- âš  éƒ¨åˆ†æ•°æ®éœ€è¦æ‚¨ç¡®è®¤å‡†ç¡®æ€§")
        lines.append("")

        # 2. å…³é”®ä¿¡æ¯æ± æ¦‚è§ˆ
        for category, label in [
            ("achievement", "ğŸ† å…³é”®æˆæœ"),
            ("data", "ğŸ“Š å…³é”®æ•°æ®"),
            ("problem", "âš ï¸ é—®é¢˜/é£é™©"),
            ("plan", "ğŸ“… ä¸‹ä¸€æ­¥è®¡åˆ’"),
        ]:
            items = pool.get(category, [])
            if items:
                lines.append(f"### {label} ({len(items)}é¡¹)")
                for item in items[:5]:
                    conf_icon = {"high": "âœ…", "medium": "âš¡", "low": "â“"}.get(
                        item.get("confidence", "medium"), "âš¡"
                    )
                    lines.append(f"  {conf_icon} {item['content']}")
                    if item.get("data_validation"):
                        for dv in item["data_validation"]:
                            if not dv.get("verified"):
                                lines.append(f"     âš  æ•°æ® '{dv['value']}' æœªåœ¨åŸæ–‡ä¸­ç²¾ç¡®åŒ¹é…ï¼Œè¯·ç¡®è®¤")
                if len(items) > 5:
                    lines.append(f"  ... è¿˜æœ‰ {len(items) - 5} é¡¹")
                lines.append("")

        # 3. è‡ªåŠ¨å†³ç­–
        if auto_decisions:
            lines.append("## ğŸ¤– Agentè‡ªåŠ¨æ¨æ–­\n")
            for d in auto_decisions:
                lines.append(f"- **{d['field']}**: {d['value']} ({d['reason']})")
            lines.append("")

        # 4. éœ€è¦ç¡®è®¤çš„é—®é¢˜
        if questions:
            lines.append("## â“ éœ€è¦æ‚¨ç¡®è®¤\n")
            for q in questions:
                lines.append(f"**{q.get('id', '')}. {q['question']}**")
                if q.get("options"):
                    for opt in q["options"]:
                        prefix = "  â†’ " if opt == q.get("default") else "    "
                        lines.append(f"{prefix}{opt}")
                if q.get("default"):
                    lines.append(f"  ï¼ˆæ¨è: {q['default']}ï¼‰")
                lines.append("")

        # 5. ä¿¡æ¯ç¼ºå£
        critical_gaps = [g for g in gaps if g.get("importance") == "high" and not g.get("can_infer")]
        if critical_gaps:
            lines.append("## âš ï¸ éœ€è¦è¡¥å……çš„ä¿¡æ¯\n")
            for g in critical_gaps:
                lines.append(f"- {g['description']}")
            lines.append("")

        return "\n".join(lines)
