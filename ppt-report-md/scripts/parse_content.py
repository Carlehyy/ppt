#!/usr/bin/env python3
"""
å†…å®¹è§£ææ¨¡å— â€” ä»å¤šä¸ªæ–‡æ¡£ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯
æ”¯æŒPDFã€Wordã€TXTã€Markdownæ ¼å¼
"""

import os
from typing import List, Dict, Any
from .llm_client import LLMClient
from .utils import detect_file_type, validate_files, load_prompt


class ContentParser:
    """å†…å®¹è§£æå™¨"""

    def __init__(self, llm: LLMClient):
        self.llm = llm

    def parse(self, file_paths: List[str]) -> Dict[str, Any]:
        """
        è§£æå¤šä¸ªæ–‡æ¡£ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯

        Args:
            file_paths: æ–‡æ¡£æ–‡ä»¶è·¯å¾„åˆ—è¡¨

        Returns:
            è§£æç»“æœå­—å…¸
        """
        print("ğŸ“„ å¼€å§‹è§£ææ–‡æ¡£...")

        # éªŒè¯æ–‡ä»¶
        valid_files = validate_files(file_paths)
        if not valid_files:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶å¯ä»¥è§£æ")

        # è§£ææ¯ä¸ªæ–‡æ¡£
        all_parsed = []
        for i, file_path in enumerate(valid_files, 1):
            print(f"  [{i}/{len(valid_files)}] è§£æ: {os.path.basename(file_path)}")
            parsed = self._parse_single_file(file_path)
            all_parsed.append(parsed)

        # å¤šæ–‡æ¡£å…³è”åˆ†æ
        if len(all_parsed) > 1:
            print("  ğŸ”— åˆ†æå¤šæ–‡æ¡£å…³è”å…³ç³»...")
            combined_result = self._combine_multi_documents(all_parsed)
        else:
            combined_result = all_parsed[0]

        print("âœ… æ–‡æ¡£è§£æå®Œæˆ\n")
        return combined_result

    def _parse_single_file(self, file_path: str) -> Dict[str, Any]:
        """è§£æå•ä¸ªæ–‡ä»¶"""
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = self._read_file_content(file_path)

        # è°ƒç”¨LLMè¿›è¡Œå››å±‚é€’è¿›åˆ†æ
        prompt = load_prompt("content_analysis", document_content=content)
        result = self.llm.call_llm(prompt, response_json=True)

        # æ·»åŠ æ–‡ä»¶å…ƒä¿¡æ¯
        result["file_info"] = {
            "file_path": file_path,
            "file_name": os.path.basename(file_path),
            "file_type": detect_file_type(file_path)
        }

        return result

    def _read_file_content(self, file_path: str) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        file_type = detect_file_type(file_path)

        if file_type == "pdf":
            return self._read_pdf(file_path)
        elif file_type == "docx":
            return self._read_docx(file_path)
        elif file_type in ("txt", "markdown"):
            return self._read_text(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")

    def _read_pdf(self, file_path: str) -> str:
        """è¯»å–PDFæ–‡ä»¶"""
        try:
            from PyPDF2 import PdfReader
            reader = PdfReader(file_path)
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n\n"
            return text.strip()
        except ImportError:
            raise ImportError("è¯·å®‰è£…PyPDF2: pip install PyPDF2")

    def _read_docx(self, file_path: str) -> str:
        """è¯»å–Wordæ–‡æ¡£"""
        try:
            from docx import Document
            doc = Document(file_path)
            text = ""
            for para in doc.paragraphs:
                text += para.text + "\n"
            return text.strip()
        except ImportError:
            raise ImportError("è¯·å®‰è£…python-docx: pip install python-docx")

    def _read_text(self, file_path: str) -> str:
        """è¯»å–çº¯æ–‡æœ¬æ–‡ä»¶"""
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()

    def _combine_multi_documents(self, parsed_docs: List[Dict]) -> Dict[str, Any]:
        """åˆå¹¶å¤šä¸ªæ–‡æ¡£çš„è§£æç»“æœ"""
        # æ„å»ºåˆå¹¶æç¤ºè¯
        docs_summary = []
        for i, doc in enumerate(parsed_docs, 1):
            profile = doc.get("document_profile", {})
            docs_summary.append(f"æ–‡æ¡£{i}: {profile.get('core_topic', 'æœªçŸ¥ä¸»é¢˜')}")

        # åˆå¹¶æ‰€æœ‰ä¿¡æ¯è¦ç´ 
        combined_elements = {
            "background_goals": [],
            "key_achievements": [],
            "methods_process": [],
            "issues_challenges": [],
            "data_metrics": [],
            "key_conclusions": [],
            "next_steps": []
        }

        for doc in parsed_docs:
            elements = doc.get("information_elements", {})
            for key in combined_elements.keys():
                combined_elements[key].extend(elements.get(key, []))

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„æ¡†æ¶ä½œä¸ºåŸºç¡€
        result = parsed_docs[0].copy()
        result["information_elements"] = combined_elements
        result["source_documents"] = [doc.get("file_info", {}) for doc in parsed_docs]

        # æ›´æ–°è·¨æ–‡æ¡£åˆ†æ
        if "cross_document_analysis" in result:
            result["cross_document_analysis"]["document_count"] = len(parsed_docs)
            result["cross_document_analysis"]["documents_summary"] = docs_summary

        return result
